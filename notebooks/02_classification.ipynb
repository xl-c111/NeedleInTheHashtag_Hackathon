{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eSafety Hackathon - Classification Approaches\n",
    "\n",
    "This notebook demonstrates different approaches to classifying personas.\n",
    "\n",
    "## Approaches Covered\n",
    "1. **Perspective API** - Quick toxicity baseline\n",
    "2. **LLM Classification** - Flexible, prompt-based\n",
    "3. **Embedding + Clustering** - Unsupervised grouping\n",
    "4. **Keyword/Pattern Matching** - Fast, explainable baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# Sample data for testing\n",
    "test_messages = [\n",
    "    \"I just got promoted to VP after only 6 months! Hard work pays off. Thoughts? ðŸš€ðŸ’ª\",\n",
    "    \"I hate my body. Why can't I look like the models I see online?\",\n",
    "    \"Women only want one thing and it's not personality. The game is rigged.\",\n",
    "    \"Had a nice walk in the park today. The weather was perfect! ðŸŒ³\",\n",
    "    \"I'm HUMBLED to announce that I've been named a Top 30 Under 30 Thought Leader in Digital Transformation. None of this would be possible without my amazing team. Agree that teamwork makes the dream work? ðŸ™âœ¨\",\n",
    "    \"Skipped dinner again. Need to lose these last 5 pounds before the wedding.\",\n",
    "    \"Society is designed to keep us down. Wake up.\",\n",
    "    \"Just finished a great book! Would recommend to anyone interested in history.\"\n",
    "]\n",
    "\n",
    "print(f\"Testing with {len(test_messages)} sample messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: Keyword/Pattern Matching (Fast Baseline)\n",
    "\n",
    "Simple but explainable. Good starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def classify_by_keywords(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Simple keyword-based classification.\n",
    "    Returns dict with scores for each persona.\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    scores = {\n",
    "        'linkedin_lunatic': 0,\n",
    "        'body_dysmorphia': 0,\n",
    "        'incel': 0,\n",
    "        'normal': 0\n",
    "    }\n",
    "    \n",
    "    # LinkedIn Lunatic indicators\n",
    "    linkedin_patterns = [\n",
    "        r'agree\\s*\\?',\n",
    "        r'thoughts\\s*\\?',\n",
    "        r\"i('m| am) (humbled|honored|blessed|grateful)\",\n",
    "        r'thought leader',\n",
    "        r'game.?changer',\n",
    "        r'dream job',\n",
    "        r'promoted',\n",
    "        r'ðŸš€|ðŸ’ª|ðŸ™|âœ¨',\n",
    "        r'hard work pays off',\n",
    "        r'none of this would be possible'\n",
    "    ]\n",
    "    for pattern in linkedin_patterns:\n",
    "        if re.search(pattern, text_lower) or re.search(pattern, text):\n",
    "            scores['linkedin_lunatic'] += 1\n",
    "    \n",
    "    # Body dysmorphia indicators\n",
    "    body_patterns = [\n",
    "        r'hate my body',\n",
    "        r'(wish i|if only i) (looked|was|could be)',\n",
    "        r'(skipped|skip) (lunch|dinner|breakfast|meals?)',\n",
    "        r'need to lose',\n",
    "        r'too (fat|skinny|ugly)',\n",
    "        r\"can't look like\",\n",
    "        r'(thinner|skinnier|prettier|more attractive)',\n",
    "        r'fasting',\n",
    "        r'calories'\n",
    "    ]\n",
    "    for pattern in body_patterns:\n",
    "        if re.search(pattern, text_lower):\n",
    "            scores['body_dysmorphia'] += 1\n",
    "    \n",
    "    # Incel indicators (be careful with these - context matters)\n",
    "    incel_patterns = [\n",
    "        r'(women|females?) only want',\n",
    "        r'rigged',\n",
    "        r'society (is|designed to)',\n",
    "        r'wake up',\n",
    "        r'blackpill',\n",
    "        r'chad',\n",
    "        r'normie',\n",
    "        r'keep us down'\n",
    "    ]\n",
    "    for pattern in incel_patterns:\n",
    "        if re.search(pattern, text_lower):\n",
    "            scores['incel'] += 1\n",
    "    \n",
    "    # Determine primary category\n",
    "    max_score = max(scores.values())\n",
    "    if max_score == 0:\n",
    "        scores['normal'] = 1\n",
    "        primary = 'normal'\n",
    "    else:\n",
    "        primary = max(scores, key=scores.get)\n",
    "    \n",
    "    return {\n",
    "        'primary_category': primary,\n",
    "        'scores': scores,\n",
    "        'confidence': max_score / 5  # Normalize to 0-1 range\n",
    "    }\n",
    "\n",
    "# Test it\n",
    "print(\"Keyword-based Classification Results:\\n\")\n",
    "for msg in test_messages:\n",
    "    result = classify_by_keywords(msg)\n",
    "    print(f\"Category: {result['primary_category']} (conf: {result['confidence']:.2f})\")\n",
    "    print(f\"  Text: {msg[:80]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: Perspective API (Toxicity Baseline)\n",
    "\n",
    "**Note**: Requires API key. Get one at https://developers.perspectiveapi.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run if you have a Perspective API key\n",
    "\n",
    "# from src.classifiers.api_classifiers import PerspectiveAPI\n",
    "\n",
    "# try:\n",
    "#     perspective = PerspectiveAPI()\n",
    "#     \n",
    "#     print(\"Perspective API Analysis:\\n\")\n",
    "#     for msg in test_messages[:3]:  # Limit to avoid rate limits\n",
    "#         scores = perspective.analyze(msg, attributes=['TOXICITY', 'INSULT', 'IDENTITY_ATTACK'])\n",
    "#         print(f\"Text: {msg[:60]}...\")\n",
    "#         for attr, score in scores.items():\n",
    "#             print(f\"  {attr}: {score:.3f}\")\n",
    "#         print()\n",
    "# except Exception as e:\n",
    "#     print(f\"Perspective API not configured: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 3: LLM Classification (Flexible & Powerful)\n",
    "\n",
    "Use GPT-4 or Claude for nuanced classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run if you have an OpenAI or Anthropic API key\n",
    "\n",
    "# from src.classifiers.api_classifiers import LLMClassifier\n",
    "\n",
    "# try:\n",
    "#     # Use OpenAI (gpt-4o-mini is fast and cheap)\n",
    "#     classifier = LLMClassifier(provider='openai', model='gpt-4o-mini')\n",
    "#     \n",
    "#     # Or use Anthropic\n",
    "#     # classifier = LLMClassifier(provider='anthropic')\n",
    "#     \n",
    "#     print(\"LLM Classification Results:\\n\")\n",
    "#     for msg in test_messages:\n",
    "#         result = classifier.classify_persona(msg)\n",
    "#         print(f\"Category: {result.get('category')} (conf: {result.get('confidence', 'N/A')})\")\n",
    "#         print(f\"  Indicators: {result.get('indicators', [])}\")\n",
    "#         print(f\"  Text: {msg[:60]}...\\n\")\n",
    "# except Exception as e:\n",
    "#     print(f\"LLM not configured: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 4: Embedding + Clustering\n",
    "\n",
    "Group similar messages together without predefined categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This uses sentence-transformers (runs locally, no API needed)\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.decomposition import PCA\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Load model (first time will download ~90MB)\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Get embeddings\n",
    "    embeddings = model.encode(test_messages)\n",
    "    print(f\"Embedding shape: {embeddings.shape}\")\n",
    "    \n",
    "    # Cluster\n",
    "    n_clusters = 3\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(embeddings)\n",
    "    \n",
    "    # Visualize with PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    embeddings_2d = pca.fit_transform(embeddings)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=labels, cmap='viridis', s=100)\n",
    "    \n",
    "    # Add labels\n",
    "    for i, txt in enumerate(test_messages):\n",
    "        plt.annotate(f\"{i}: {txt[:30]}...\", (embeddings_2d[i, 0], embeddings_2d[i, 1]), fontsize=8)\n",
    "    \n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.title('Message Embeddings (PCA)')\n",
    "    plt.show()\n",
    "    \n",
    "    # Show clusters\n",
    "    print(\"\\nCluster assignments:\")\n",
    "    for cluster in range(n_clusters):\n",
    "        print(f\"\\nCluster {cluster}:\")\n",
    "        for i, (msg, label) in enumerate(zip(test_messages, labels)):\n",
    "            if label == cluster:\n",
    "                print(f\"  - {msg[:70]}...\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"sentence-transformers not installed. Run: pip install sentence-transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Approaches\n",
    "\n",
    "Best results often come from combining multiple signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_ensemble(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Combine multiple classification signals.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Keyword baseline\n",
    "    keyword_result = classify_by_keywords(text)\n",
    "    results['keyword'] = keyword_result['primary_category']\n",
    "    results['keyword_confidence'] = keyword_result['confidence']\n",
    "    \n",
    "    # 2. Add Perspective API scores if available\n",
    "    # results['toxicity'] = perspective_score\n",
    "    \n",
    "    # 3. Add LLM classification if available\n",
    "    # results['llm'] = llm_result\n",
    "    \n",
    "    # 4. Feature-based signals\n",
    "    from src.utils.text_utils import count_features, detect_patterns\n",
    "    features = count_features(text)\n",
    "    patterns = detect_patterns(text)\n",
    "    \n",
    "    results['emoji_heavy'] = features.get('emoji_count', 0) > 3\n",
    "    results['shouting'] = patterns.get('shouting', False)\n",
    "    results['engagement_bait'] = patterns.get('engagement_bait', False)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test ensemble\n",
    "print(\"Ensemble Classification:\\n\")\n",
    "for msg in test_messages:\n",
    "    result = classify_ensemble(msg)\n",
    "    print(f\"Text: {msg[:60]}...\")\n",
    "    for key, value in result.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Up\n",
    "\n",
    "Tips for processing millions of messages:\n",
    "\n",
    "1. **Start with keyword baseline** - Fast, runs on everything\n",
    "2. **Use embeddings for clustering** - Can process locally, no API limits\n",
    "3. **Reserve LLM calls for uncertain cases** - Most expensive, use strategically\n",
    "4. **Batch processing** - Use async/parallel where possible\n",
    "5. **Sample first** - Test on 1000 messages before running on millions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Processing a large dataset efficiently\n",
    "\n",
    "def process_large_dataset(df: pd.DataFrame, text_column: str = 'text') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process a large dataset with tiered classification.\n",
    "    \"\"\"\n",
    "    from tqdm import tqdm\n",
    "    tqdm.pandas()\n",
    "    \n",
    "    # Step 1: Fast keyword classification for everything\n",
    "    print(\"Step 1: Keyword classification...\")\n",
    "    df['keyword_result'] = df[text_column].progress_apply(classify_by_keywords)\n",
    "    df['primary_category'] = df['keyword_result'].apply(lambda x: x['primary_category'])\n",
    "    df['confidence'] = df['keyword_result'].apply(lambda x: x['confidence'])\n",
    "    \n",
    "    # Step 2: Flag uncertain cases for LLM review\n",
    "    uncertain_mask = df['confidence'] < 0.4\n",
    "    print(f\"\\nStep 2: {uncertain_mask.sum()} uncertain cases flagged for LLM review\")\n",
    "    \n",
    "    # Step 3: LLM classification on uncertain cases (if API available)\n",
    "    # df.loc[uncertain_mask, 'llm_result'] = ...\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Test on sample\n",
    "sample_df = pd.DataFrame({'text': test_messages})\n",
    "result_df = process_large_dataset(sample_df)\n",
    "result_df[['text', 'primary_category', 'confidence']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Get the real data** from the hackathon platform\n",
    "2. **Explore unique vocabulary** - What words/phrases appear only in certain personas?\n",
    "3. **Look at user-level patterns** - Aggregate messages per user for more signal\n",
    "4. **Build a simple UI** - For the demo, consider Streamlit or Gradio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
