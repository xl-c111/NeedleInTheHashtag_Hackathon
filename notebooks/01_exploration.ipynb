{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eSafety Hackathon - Data Exploration\n",
    "\n",
    "This notebook provides a starting point for exploring the hackathon data.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# Our utilities\n",
    "from src.utils.text_utils import clean_text, count_features, detect_patterns, extract_emojis\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Setup complete! âœ…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Update the path below to point to your hackathon data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update this path to your data file\n",
    "DATA_PATH = '../data/raw/messages.csv'  # or .json, .parquet, etc.\n",
    "\n",
    "# Example loading (uncomment the one that matches your data format)\n",
    "# df = pd.read_csv(DATA_PATH)\n",
    "# df = pd.read_json(DATA_PATH)\n",
    "# df = pd.read_parquet(DATA_PATH)\n",
    "\n",
    "# For now, create sample data to test the pipeline\n",
    "sample_data = [\n",
    "    {\"id\": 1, \"user\": \"user_001\", \"text\": \"I just landed my DREAM job after being rejected 47 times. Never give up! Agree? ðŸš€\"},\n",
    "    {\"id\": 2, \"user\": \"user_002\", \"text\": \"I wish I could look like her. Skipped lunch again today.\"},\n",
    "    {\"id\": 3, \"user\": \"user_003\", \"text\": \"Had a great coffee this morning! â˜•\"},\n",
    "    {\"id\": 4, \"user\": \"user_004\", \"text\": \"Nobody understands. Society is rigged against people like us.\"},\n",
    "    {\"id\": 5, \"user\": \"user_001\", \"text\": \"I'm HUMBLED to announce I've been selected as a thought leader in disruption. ðŸ™\"},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(sample_data)\n",
    "print(f\"Loaded {len(df)} messages\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic stats\n",
    "print(f\"Total messages: {len(df):,}\")\n",
    "print(f\"Unique users: {df['user'].nunique():,}\")\n",
    "print(f\"\\nMessages per user:\")\n",
    "print(df['user'].value_counts().describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text features\n",
    "df['features'] = df['text'].apply(count_features)\n",
    "\n",
    "# Expand features into columns\n",
    "features_df = pd.json_normalize(df['features'])\n",
    "df = pd.concat([df, features_df], axis=1)\n",
    "\n",
    "# Show feature distributions\n",
    "print(\"Text Feature Statistics:\")\n",
    "df[['word_count', 'emoji_count', 'caps_ratio', 'exclamation_count']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect patterns\n",
    "df['patterns'] = df['text'].apply(detect_patterns)\n",
    "\n",
    "# Show which patterns were detected\n",
    "for idx, row in df.iterrows():\n",
    "    detected = [k for k, v in row['patterns'].items() if v]\n",
    "    if detected:\n",
    "        print(f\"Message {row['id']}: {detected}\")\n",
    "        print(f\"  Text: {row['text'][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Level Analysis\n",
    "\n",
    "For persona classification, we might want to aggregate at the user level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by user\n",
    "user_stats = df.groupby('user').agg({\n",
    "    'text': 'count',\n",
    "    'word_count': 'mean',\n",
    "    'emoji_count': 'sum',\n",
    "    'exclamation_count': 'sum',\n",
    "    'caps_ratio': 'mean'\n",
    "}).rename(columns={'text': 'message_count'})\n",
    "\n",
    "user_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word count distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "axes[0, 0].hist(df['word_count'], bins=30, edgecolor='black')\n",
    "axes[0, 0].set_title('Word Count Distribution')\n",
    "axes[0, 0].set_xlabel('Words per message')\n",
    "\n",
    "axes[0, 1].hist(df['emoji_count'], bins=20, edgecolor='black')\n",
    "axes[0, 1].set_title('Emoji Count Distribution')\n",
    "axes[0, 1].set_xlabel('Emojis per message')\n",
    "\n",
    "axes[1, 0].hist(df['caps_ratio'], bins=20, edgecolor='black')\n",
    "axes[1, 0].set_title('Caps Ratio Distribution')\n",
    "axes[1, 0].set_xlabel('Ratio of uppercase letters')\n",
    "\n",
    "# Messages per user\n",
    "df['user'].value_counts().head(20).plot(kind='bar', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Top 20 Users by Message Count')\n",
    "axes[1, 1].set_xlabel('User')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Common Words/Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Get all words\n",
    "all_text = ' '.join(df['text'].apply(lambda x: clean_text(x, remove_punctuation=True)))\n",
    "words = all_text.split()\n",
    "\n",
    "# Filter out common stop words\n",
    "stop_words = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "              'to', 'of', 'and', 'in', 'that', 'it', 'for', 'on', 'with', 'as', 'at',\n",
    "              'by', 'this', 'from', 'or', 'but', 'not', 'you', 'your', 'i', 'my', 'me',\n",
    "              'we', 'our', 'they', 'their', 'he', 'she', 'his', 'her', 'have', 'has'}\n",
    "\n",
    "filtered_words = [w for w in words if w not in stop_words and len(w) > 2]\n",
    "\n",
    "print(\"Top 30 words:\")\n",
    "for word, count in Counter(filtered_words).most_common(30):\n",
    "    print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Run the classification notebook** (`02_classification.ipynb`) to test different approaches\n",
    "2. **Explore user patterns** - Do certain users consistently post in specific styles?\n",
    "3. **Look for clusters** - Use embeddings to group similar messages\n",
    "4. **Check for temporal patterns** - If timestamps available, look for posting patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
